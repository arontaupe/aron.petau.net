---
title                 :  "Bachelor Thesis"
date                  :   2022-03-01 14:39:27 +0100
header:
  image       : "/assets/images/rt_choice_corr_by_condition.png"
  overlay_filter      : 0.2
  credit              : "Aron Petau"

tags:
  - audiovisual asynchrony
  - autism
  - JavaScript
  - latency
  - latex
  - multi-sensory integration
  - Pavlovia
  - psychoJS
  - Psycholinguistics
  - Python
  - R
  - RT
  - seaborn
  - sensory hypersensitivity
  - smart hearing protection
  - bachelor thesis
  - university
---

## Online Linguistic RT Study using reaction time

Last year, I wrote my thesis during the pandemic. With the struggles our university had transitioning to online teaching I selected a guided topic, although my initial dream was to start writing about my proposed plan of automated plastic recycling. You can read more about that here: TODO

I chose a project that wanted to examine the possibilities of a novel smart hearing protection device specifically designed for auditory hypersensitivity, which is often, but not always and not exclusively a phenomenon visible in people with autism spectrum disorder. 

A common reaction to this elevated sensitivity is stress and avoidance behaviour, often leading to very awkward social situations and really impairing the ability to take part in social situations. 

Schools are one such social situation and we all know the stress a noisy classroom can produce. Concentration is gone, and education, as well as essential skills like language reproduction suffer. 

There is lots of prior research on these fields, and there is some evidence that sensory information in people on the Autism spectrum is processed differently than in a neurotypical brain. It seems that a certain adaptability, needed to overcome noise issues and bridge asynchronity between auditory and visual sensory input, is reduced in some people on the Autism Spectrum. 

In essence, my experiment was responsible for looking at neurotypical people and measure any effect on language perception produced by varying the delay between auditory and visual input, as well as the loudness. 

Here, I had the possibility to conduct an entire reaction-time-based experiment with over 70 participants and went through all the struggles that come with proper science. 
I did an extensive literature research, coded the experiment, and learned a lot about the reasons nobody really ever does reaction time based studies like this via a common internet browser. 
It was an almost 9 months long learning experience full of doing things I had never done before. 

I learned and got to love writing in Latex, had to learn JavaScript for efficient serving of the stimuli, and R for the statistical analysis. I also got to brush up my data visualization skills in python and made some pretty graphs of the results.

The experiment is still working and online if you want to have a look at it. Be mindful though that in measuring reaction speed every millisecond is important, which is why it makes heavy use of your browsing cache and has been known to crash and defeat some not so tough computers. 

[Try out the experiment yourself](https://moryscarter.com/vespr/pavlovia.php?folder=arontaupe&experiment=av_experiment/&id=public&researcher=aron){: .btn .btn--large}

Even with writing alone I had extensive helpful feedback from my supervisors and learned a lot about scientific processes and associated considerations. 

There was always the next unsolvable problem. Just one example was scientificity and ethical considerations clashing, data privacy against accuracy of results. Since the machines participants participated on, were private devices, I was unable to know important data like their internet speed and provider, their type of GPU and type of external hardware. Turns out, for an auditory experiment, the type and setup of the speakers does play an important role and influences response speed. 



The final version of my thesis has something around 80 pages, much of it utterly boring, but nevertheless important, statistical analyses. If you really want, you can have a look at the whole thing here: 


[Read the original Thesis](https://github.com/arontaupe/asynchrony_thesis/blob/main/AronPetauBAThesis.pdf
){: .btn .btn--large}

I am a fan and proponent of open source and open science practices. 
So here you can also find the rest of the project with the original source code. 
I am not yet where I want to be with my documentation practices, and it scares me a bit that anyone can now have full grasp of all the mistakes I did, but I am throwing this out there as a practice step. I learned and gained a lot from looking at other peoples projects and I strive to be open about my processes too. 

The original video stimuli are not mine and I have no right releasing them, so they are omitted here.

[Find the complete Repo on Github](https://github.com/arontaupe/asynchrony_thesis
){: .btn .btn--large}

